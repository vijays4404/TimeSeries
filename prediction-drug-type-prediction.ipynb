{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing library\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-02T12:42:45.584701Z","iopub.execute_input":"2022-03-02T12:42:45.585903Z","iopub.status.idle":"2022-03-02T12:42:45.592935Z","shell.execute_reply.started":"2022-03-02T12:42:45.585854Z","shell.execute_reply":"2022-03-02T12:42:45.591890Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#loading the dataset\ndf=pd.read_csv('../input/memory-test-on-drugged-islanders-data/Islander_data.csv')\n#showing the dataset\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:43:17.649766Z","iopub.execute_input":"2022-03-02T12:43:17.650186Z","iopub.status.idle":"2022-03-02T12:43:17.694227Z","shell.execute_reply.started":"2022-03-02T12:43:17.650154Z","shell.execute_reply":"2022-03-02T12:43:17.693450Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     first_name  last_name  age Happy_Sad_group  Dosage Drug  \\\n0       Bastian   Carrasco   25               H       1    A   \n1          Evan   Carrasco   52               S       1    A   \n2     Florencia   Carrasco   29               H       1    A   \n3         Holly   Carrasco   50               S       1    A   \n4        Justin   Carrasco   52               H       1    A   \n..          ...        ...  ...             ...     ...  ...   \n193       Jacob      Novak   52               H       3    T   \n194         Teo    Steiner   41               S       3    T   \n195   Alexander  Takahashi   54               S       3    T   \n196  Alexandere  Takahashi   40               H       3    T   \n197       Chloe  Takahashi   32               S       3    T   \n\n     Mem_Score_Before  Mem_Score_After  Diff  \n0                63.5             61.2  -2.3  \n1                41.6             40.7  -0.9  \n2                59.7             55.1  -4.6  \n3                51.7             51.2  -0.5  \n4                47.0             47.1   0.1  \n..                ...              ...   ...  \n193              71.3             74.3   3.0  \n194              72.5             70.4  -2.1  \n195              30.8             33.1   2.3  \n196              53.6             53.8   0.2  \n197              43.1             42.1  -1.0  \n\n[198 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>age</th>\n      <th>Happy_Sad_group</th>\n      <th>Dosage</th>\n      <th>Drug</th>\n      <th>Mem_Score_Before</th>\n      <th>Mem_Score_After</th>\n      <th>Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bastian</td>\n      <td>Carrasco</td>\n      <td>25</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>63.5</td>\n      <td>61.2</td>\n      <td>-2.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Evan</td>\n      <td>Carrasco</td>\n      <td>52</td>\n      <td>S</td>\n      <td>1</td>\n      <td>A</td>\n      <td>41.6</td>\n      <td>40.7</td>\n      <td>-0.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Florencia</td>\n      <td>Carrasco</td>\n      <td>29</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>59.7</td>\n      <td>55.1</td>\n      <td>-4.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Holly</td>\n      <td>Carrasco</td>\n      <td>50</td>\n      <td>S</td>\n      <td>1</td>\n      <td>A</td>\n      <td>51.7</td>\n      <td>51.2</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Justin</td>\n      <td>Carrasco</td>\n      <td>52</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>47.1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>Jacob</td>\n      <td>Novak</td>\n      <td>52</td>\n      <td>H</td>\n      <td>3</td>\n      <td>T</td>\n      <td>71.3</td>\n      <td>74.3</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>Teo</td>\n      <td>Steiner</td>\n      <td>41</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>72.5</td>\n      <td>70.4</td>\n      <td>-2.1</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>Alexander</td>\n      <td>Takahashi</td>\n      <td>54</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>30.8</td>\n      <td>33.1</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Alexandere</td>\n      <td>Takahashi</td>\n      <td>40</td>\n      <td>H</td>\n      <td>3</td>\n      <td>T</td>\n      <td>53.6</td>\n      <td>53.8</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Chloe</td>\n      <td>Takahashi</td>\n      <td>32</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>43.1</td>\n      <td>42.1</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>198 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#getting informatin about the dataset\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:43:36.829670Z","iopub.execute_input":"2022-03-02T12:43:36.830093Z","iopub.status.idle":"2022-03-02T12:43:36.853556Z","shell.execute_reply.started":"2022-03-02T12:43:36.830063Z","shell.execute_reply":"2022-03-02T12:43:36.852459Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 198 entries, 0 to 197\nData columns (total 9 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   first_name        198 non-null    object \n 1   last_name         198 non-null    object \n 2   age               198 non-null    int64  \n 3   Happy_Sad_group   198 non-null    object \n 4   Dosage            198 non-null    int64  \n 5   Drug              198 non-null    object \n 6   Mem_Score_Before  198 non-null    float64\n 7   Mem_Score_After   198 non-null    float64\n 8   Diff              198 non-null    float64\ndtypes: float64(3), int64(2), object(4)\nmemory usage: 14.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"#creating one_hot encode \ndef onehot_encode(df,column):\n    df=df.copy()\n    dummies=pd.get_dummies(df[column],prefix=column)\n    if len(df[column].unique())==2:\n        dummies=dummies.drop(dummies.columns[0],axis=1)\n        \n    df=pd.concat([df,dummies],axis=1)\n    df=df.drop(column,axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:53:25.664807Z","iopub.execute_input":"2022-03-02T12:53:25.665104Z","iopub.status.idle":"2022-03-02T12:53:25.671254Z","shell.execute_reply.started":"2022-03-02T12:53:25.665070Z","shell.execute_reply":"2022-03-02T12:53:25.670427Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#getting length of unique value in each column\n{column:len(x[column].unique()) for column in x.columns}","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:46:03.689685Z","iopub.execute_input":"2022-03-02T12:46:03.690020Z","iopub.status.idle":"2022-03-02T12:46:03.702436Z","shell.execute_reply.started":"2022-03-02T12:46:03.689981Z","shell.execute_reply":"2022-03-02T12:46:03.701329Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'first_name': 139,\n 'last_name': 18,\n 'age': 45,\n 'Happy_Sad_group': 2,\n 'Dosage': 3,\n 'Drug': 3,\n 'Mem_Score_Before': 162,\n 'Mem_Score_After': 151,\n 'Diff': 142}"},"metadata":{}}]},{"cell_type":"code","source":"#preprocessing \ndef preprocess_inputs(df):\n    df=df.copy()\n    #One hot encode categorical features\n    for column in ['first_name','last_name','Happy_Sad_group']:\n        df=onehot_encode(df,column)\n    #Split df into x and y\n    y=df['Drug']\n    x=df.drop('Drug',axis=1)\n    #train_test_split\n    x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7,random_state=1)\n    #scaling x\n    scaler=StandardScaler()\n    scaler.fit(x_train)\n    x_train=pd.DataFrame(scaler.transform(x_train),columns=x.columns,index=x_train.index)\n    x_test=pd.DataFrame(scaler.transform(x_test),columns=x.columns,index=x_test.index)\n    \n    return x_train,x_test,y_train,y_test","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:58:25.213768Z","iopub.execute_input":"2022-03-02T12:58:25.214638Z","iopub.status.idle":"2022-03-02T12:58:25.221989Z","shell.execute_reply.started":"2022-03-02T12:58:25.214597Z","shell.execute_reply":"2022-03-02T12:58:25.221305Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#calling the function\nx_train,x_test,y_train,y_test=preprocess_inputs(df)\nx_train","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:58:38.325141Z","iopub.execute_input":"2022-03-02T12:58:38.326148Z","iopub.status.idle":"2022-03-02T12:58:38.394496Z","shell.execute_reply.started":"2022-03-02T12:58:38.326099Z","shell.execute_reply":"2022-03-02T12:58:38.393681Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"          age    Dosage  Mem_Score_Before  Mem_Score_After      Diff  \\\n124 -0.302247  1.206716          0.249183        -0.151850 -0.594735   \n97   0.909251  0.025675          1.221038         1.038471 -0.092208   \n42   1.428464  0.025675          0.438505        -0.684661 -1.707473   \n17  -1.167603 -1.155366          0.413262         0.698379  0.518003   \n5   -0.215712 -1.155366          0.564720        -0.117841 -0.989577   \n..        ...       ...               ...              ...       ...   \n133 -0.215712 -1.155366         -0.665875        -0.752679 -0.244761   \n137  1.947677 -1.155366          2.432701         2.370498  0.293661   \n72   1.082322 -1.155366          0.552099         0.069210 -0.675498   \n140 -0.475319 -1.155366         -0.533349        -1.115444 -1.007525   \n37  -1.254139  0.025675         -0.470242        -0.610974 -0.298603   \n\n     first_name_Aaron  first_name_Adam  first_name_Ai  first_name_Akane  \\\n124               0.0        -0.121268            0.0         -0.121268   \n97                0.0        -0.121268            0.0         -0.121268   \n42                0.0        -0.121268            0.0         -0.121268   \n17                0.0        -0.121268            0.0         -0.121268   \n5                 0.0        -0.121268            0.0         -0.121268   \n..                ...              ...            ...               ...   \n133               0.0        -0.121268            0.0         -0.121268   \n137               0.0        -0.121268            0.0         -0.121268   \n72                0.0        -0.121268            0.0         -0.121268   \n140               0.0        -0.121268            0.0         -0.121268   \n37                0.0        -0.121268            0.0         -0.121268   \n\n     first_name_Akira  ...  last_name_Lopez  last_name_McCarthy  \\\n124               0.0  ...         2.761340            -0.23116   \n97                0.0  ...        -0.362143            -0.23116   \n42                0.0  ...        -0.362143            -0.23116   \n17                0.0  ...        -0.362143            -0.23116   \n5                 0.0  ...        -0.362143            -0.23116   \n..                ...  ...              ...                 ...   \n133               0.0  ...        -0.362143            -0.23116   \n137               0.0  ...        -0.362143            -0.23116   \n72                0.0  ...        -0.362143            -0.23116   \n140               0.0  ...        -0.362143            -0.23116   \n37                0.0  ...         2.761340            -0.23116   \n\n     last_name_Morin  last_name_Novak  last_name_Price  last_name_Rodriguez  \\\n124        -0.085436        -0.085436              0.0            -0.085436   \n97         -0.085436        -0.085436              0.0            -0.085436   \n42         -0.085436        -0.085436              0.0            -0.085436   \n17         11.704700        -0.085436              0.0            -0.085436   \n5          -0.085436        -0.085436              0.0            -0.085436   \n..               ...              ...              ...                  ...   \n133        -0.085436        -0.085436              0.0            -0.085436   \n137        -0.085436        -0.085436              0.0            -0.085436   \n72         -0.085436        -0.085436              0.0            -0.085436   \n140        -0.085436        -0.085436              0.0            -0.085436   \n37         -0.085436        -0.085436              0.0            -0.085436   \n\n     last_name_Steiner  last_name_Summers  last_name_Takahashi  \\\n124          -0.264135          -0.264135            -0.336011   \n97           -0.264135          -0.264135            -0.336011   \n42           -0.264135           3.785939            -0.336011   \n17           -0.264135          -0.264135            -0.336011   \n5            -0.264135          -0.264135            -0.336011   \n..                 ...                ...                  ...   \n133          -0.264135          -0.264135            -0.336011   \n137          -0.264135          -0.264135            -0.336011   \n72           -0.264135          -0.264135            -0.336011   \n140          -0.264135          -0.264135            -0.336011   \n37           -0.264135          -0.264135            -0.336011   \n\n     Happy_Sad_group_S  \n124                1.0  \n97                 1.0  \n42                 1.0  \n17                -1.0  \n5                  1.0  \n..                 ...  \n133                1.0  \n137               -1.0  \n72                 1.0  \n140                1.0  \n37                -1.0  \n\n[138 rows x 163 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>Dosage</th>\n      <th>Mem_Score_Before</th>\n      <th>Mem_Score_After</th>\n      <th>Diff</th>\n      <th>first_name_Aaron</th>\n      <th>first_name_Adam</th>\n      <th>first_name_Ai</th>\n      <th>first_name_Akane</th>\n      <th>first_name_Akira</th>\n      <th>...</th>\n      <th>last_name_Lopez</th>\n      <th>last_name_McCarthy</th>\n      <th>last_name_Morin</th>\n      <th>last_name_Novak</th>\n      <th>last_name_Price</th>\n      <th>last_name_Rodriguez</th>\n      <th>last_name_Steiner</th>\n      <th>last_name_Summers</th>\n      <th>last_name_Takahashi</th>\n      <th>Happy_Sad_group_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>124</th>\n      <td>-0.302247</td>\n      <td>1.206716</td>\n      <td>0.249183</td>\n      <td>-0.151850</td>\n      <td>-0.594735</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2.761340</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>0.909251</td>\n      <td>0.025675</td>\n      <td>1.221038</td>\n      <td>1.038471</td>\n      <td>-0.092208</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1.428464</td>\n      <td>0.025675</td>\n      <td>0.438505</td>\n      <td>-0.684661</td>\n      <td>-1.707473</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>3.785939</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-1.167603</td>\n      <td>-1.155366</td>\n      <td>0.413262</td>\n      <td>0.698379</td>\n      <td>0.518003</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>11.704700</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.215712</td>\n      <td>-1.155366</td>\n      <td>0.564720</td>\n      <td>-0.117841</td>\n      <td>-0.989577</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>-0.215712</td>\n      <td>-1.155366</td>\n      <td>-0.665875</td>\n      <td>-0.752679</td>\n      <td>-0.244761</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>1.947677</td>\n      <td>-1.155366</td>\n      <td>2.432701</td>\n      <td>2.370498</td>\n      <td>0.293661</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>1.082322</td>\n      <td>-1.155366</td>\n      <td>0.552099</td>\n      <td>0.069210</td>\n      <td>-0.675498</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>-0.475319</td>\n      <td>-1.155366</td>\n      <td>-0.533349</td>\n      <td>-1.115444</td>\n      <td>-1.007525</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>-1.254139</td>\n      <td>0.025675</td>\n      <td>-0.470242</td>\n      <td>-0.610974</td>\n      <td>-0.298603</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2.761340</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>138 rows × 163 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#storing models into a dictionary\n\nmodels={\"LogisticRegression\":LogisticRegression(),\n\":DecisionTreeClassifier\":DecisionTreeClassifier(),\n\":LinearSVC\":LinearSVC(),\n\":SVC\":SVC(),\n\":MLPClassifier\":MLPClassifier(),\n\":RandomForestClassifier\":RandomForestClassifier(),\n\":GradientBoostingClassifier\":GradientBoostingClassifier(),\n\"XGBClassifier(eval_metrics=\":XGBClassifier(eval_metrics='mlogloss'),\n\"LGBMClassifier\":LGBMClassifier(),\n\":CatBoostClassifier\":CatBoostClassifier(verbose=0)}","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:06:02.995582Z","iopub.execute_input":"2022-03-02T13:06:02.996312Z","iopub.status.idle":"2022-03-02T13:06:03.003731Z","shell.execute_reply.started":"2022-03-02T13:06:02.996261Z","shell.execute_reply":"2022-03-02T13:06:03.003144Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#traning the models\nfor name,model in models.items():\n    model.fit(x_train,y_train)\n    print(name+\"trained\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:06:05.843491Z","iopub.execute_input":"2022-03-02T13:06:05.844329Z","iopub.status.idle":"2022-03-02T13:06:09.361126Z","shell.execute_reply.started":"2022-03-02T13:06:05.844291Z","shell.execute_reply":"2022-03-02T13:06:09.359956Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"LogisticRegressiontrained\n:DecisionTreeClassifiertrained\n:LinearSVCtrained\n:SVCtrained\n:MLPClassifiertrained\n:RandomForestClassifiertrained\n:GradientBoostingClassifiertrained\n[13:06:07] WARNING: ../src/learner.cc:576: \nParameters: { \"eval_metrics\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[13:06:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGBClassifier(eval_metrics=trained\nLGBMClassifiertrained\n:CatBoostClassifiertrained\n","output_type":"stream"}]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:05:00.729514Z","iopub.execute_input":"2022-03-02T13:05:00.729808Z","iopub.status.idle":"2022-03-02T13:05:00.739760Z","shell.execute_reply.started":"2022-03-02T13:05:00.729773Z","shell.execute_reply":"2022-03-02T13:05:00.739189Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"124    S\n97     S\n42     A\n17     A\n5      A\n      ..\n133    T\n137    T\n72     S\n140    T\n37     A\nName: Drug, Length: 138, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"#Getting the Results\nfor name,model in models.items():\n    print(name,':{:2f}%'.format(model.score(x_test,y_test)*100))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:07:44.619934Z","iopub.execute_input":"2022-03-02T13:07:44.620237Z","iopub.status.idle":"2022-03-02T13:07:44.803934Z","shell.execute_reply.started":"2022-03-02T13:07:44.620209Z","shell.execute_reply":"2022-03-02T13:07:44.803062Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"LogisticRegression :41.666667%\n:DecisionTreeClassifier :51.666667%\n:LinearSVC :40.000000%\n:SVC :43.333333%\n:MLPClassifier :36.666667%\n:RandomForestClassifier :46.666667%\n:GradientBoostingClassifier :50.000000%\nXGBClassifier(eval_metrics= :38.333333%\nLGBMClassifier :38.333333%\n:CatBoostClassifier :48.333333%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}